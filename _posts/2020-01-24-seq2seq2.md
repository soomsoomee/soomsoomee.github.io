---
title: "[김기현의 딥러닝을 활용한 자연어생성] seq2seq 구조(2)"
layout: post
date: 2020-01-15
image: ../assets/images/markdown.jpg
headerImage: false
tag:
- seq2seq
category: blog
author: soom
description: 김기현의 딥러닝을 활용한 자연어생성 - seq2seq 코드 복습
---

[김기현의 딥러닝을 활용한 자연어 생성](https://www.fastcampus.co.kr/data_online_dpnlg) 수업을 듣고 이번에는 seq2seq 구조 코드 구현을 복습해보았다.
(참고: [김기현 강사님 github](https://github.com/kh-kim))  
seq2seq을 사용하여 한국어 문장을 입력하고, 이를 번역한 영어 문장을 만드는 번역기를 만든다. 
이 때, teacher forcing을 사용하여 학습 시에는 디코더에서 다음 단어 예측시 ground-truth인 이전 단어를 넣어주고, 실제 추론 시에는 다음 단어 예측시 이전 예측 단어를 넣어준다.

<br/><br/>

## Import Libraries and Modules

```{.python}
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence as pack
from torch.nn.utils.rnn import pad_packed_sequence as unpack
```

<br/>

## Encoder
![seq2seq(0)](/assets/images/seq2seq복습/seq2seq(9).jpg)  

```{.python}
class Encoder(nn.Module):
  
  def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=2):
    '''
    word_vec_size : 단어의 임베딩 크기
    hidden_size: LSTM의 출력 사이즈
    n_layers: LSTM 층의 개수
    dropout_p: drop out 비율
    '''
    super(Encoder, self).__init__()
    
    # bi-directional LSTM을 사용하여 양 방향의 결과를 이어 붙일 것이므로, LSTM의 hidden size를 hidden_size/2로 설정
    # batch_first=True로 설정하여 텐서의 첫번째 차원이 배치 크기가 되도록 함
    self.rnn = nn.LSTM(
      word_vec_size,
      int(hidden_size / 2),
      num_layers = n_layers,
      bidirectional=True,
      batch_first=True,
      )
     
      def forward(self, emb):
        # |emb| = (batch_size, length:문장의 길이, word_vec_size)
        
        # 미니 배치 별로 문장 길이에 따라 문장을 재정렬함으로써 <PAD> 토큰 계산 안하고 빠른 병렬 연산 수행
        # 참고(Pytorch 의 PackedSequence object 알아보기): https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html
        if isinstance(emb, tuple):
          # 미니 배치 별로 각 문장과 길이를 받아서 pack 한다
          x, length = emb
          x = pack(x, lengths.tolist(), batch_first=True)
        else:
          x = emb
          
        y, h = self.rnn(x)
        # |y| = (batch_size, length, hidden_size)
        # -> 미니 배치별로 각 문장의 단어(time step) 별 (마지막 레이어를 거친 최종) 히든 스테이트를를 담고 있음
        # |h[0]| = (num_layers*2, batch_size, hidden_size/2)
        # -> h=(hidden, cell)이므로 h[0]은 미니 배치별, 각 레이어 별 모든 히든 스테이트를 담고 있음
        
        # 인풋이 packed된 형태라면 아웃풋도 packed된 형태이므로 풀어줌
        if isinstance(emb, tuple):
          y, _ = unpack(y, batch_first=True)
          
         return y, h
```

<br/>

## Attention
![seq2seq(0)](/assets/images/seq2seq복습/seq2seq(8).jpg)

```{.python}
class Attention(nn.Module):

  def __init__(self, hidden_size):
  '''
  hidden_size: LSTM의 출력 사이즈
  '''
  super(Attention, self).__init__()
  
  # self.linear를 통해 적절한 쿼리를 던질 수 있는 선형변환을 학습시킴
  self.linear = nn.Linear(hidden_size, hidden_size, bias=False)
  self.softmax = nn.Softmax(dim=-1)
  
  def forward(self, h_src, h_t_tgt, mask=None):
    # |h_src| = (batch_size, length, hidden_size)
    # |h_t_tgt| = (batch_size, 1, hidden_size)
    # |mask| = (batch_size, length)
    
    # 디코더의 각 단어를 넣어서 인코더에 보낼 쿼리 생성
    query = self.linear(h_t_tgt)
    # |query| = (batch_size, 1, hidden_size)
    
    # 인코더 각 단어에 곱할 attention weight
    weight = torch.bmm(query, h_src.transpose(1, 2))
    # |weight| = (batch_size, 1, length)
    
    # <PAD> 토큰에 해당하는 부분을 -inf로 바꾸어서 소프트맥스 출력값이 0이 되게 함
    # 패딩 부분에 대한 weight을 0으로 만들기
    if mask in not None:
      weight.masked_fill_(mask.unsqueeze(1), -float('inf'))
    
    weight = self.softmax(weight)
    
    # 인코더 각 단어에 attention weight을 곱해서 디코더 각 단어에 대한 context_vector 만들기
    # 인코더 단어 중 디코더 각 단어와 관련이 높은 단어는 많이, 낮은 단어는 적게 반영 
    context_vector = torch.bmm(weight, h_src)
    
    return context_vector
    
```

<br/>

## Decoder
![seq2seq(0)](/assets/images/seq2seq복습/seq2seq(10).jpg)

```{.python}
class Decoder(nn.Module):
  
  def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_o=.2):
    
    super(Decoder, self).__init__()
    
    # input 사이즈는 디코더 단어의 임베딩 차원 + context_vector의 크기(hidden_size)
    self.rnn = nn.LSTM(
      word_vec_size + hidden_size,
      hidden_size,
      num_layers = n_layers,
      dropout = dropout_p,
      bidirectional=False,
      batch_first=True,
      )
      
    def forward(self, emb_t, h_t_1_tilde, h_t_1):
      # |emb_t| = (batch_size, 1, word_vec_size)
      # |h_t_1_tilde| = (batch_size, 1, hidden_size)
      # |h_t_1[0]| = (n_layers, batch_size, hidden_size)
      
      batch_size = emb_t.size(0)
      hidden_size = h_t_1[0].size(-1)
      
      if h_t_1_tilde i None:
        h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero()
        
      x = torch.cat([emb_t, ht_t_1_tilde], dim=-1)
      
      y, h = self.rnn(x, h_t_1)
      
      return y, h
    
```
