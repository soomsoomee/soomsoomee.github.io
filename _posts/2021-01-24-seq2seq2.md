---
title: "[김기현의 딥러닝을 활용한 자연어생성] seq2seq 구조(2)"
layout: post
date: 2021-01-24
image: ../assets/images/markdown.jpg
headerImage: false
tag:
- seq2seq
category: blog
author: soom
description: 김기현의 딥러닝을 활용한 자연어생성 - seq2seq 코드 복습
---

[김기현의 딥러닝을 활용한 자연어 생성](https://www.fastcampus.co.kr/data_online_dpnlg) 수업을 듣고 이번에는 seq2seq 구조 코드 구현을 복습해보았다.
(참고: [김기현 강사님 github](https://github.com/kh-kim))  
seq2seq을 사용하여 한국어 문장을 입력하고, 이를 번역한 영어 문장을 만드는 번역기를 만든다. 
이 때, teacher forcing을 사용하여 학습 시에는 디코더에서 다음 단어 예측시 ground-truth인 이전 단어를 넣어주고, 실제 추론 시에는 다음 단어 예측시 이전 예측 단어를 넣어준다.

<br/><br/>

## Import Libraries and Modules

```{.python}
import torch
import torch.nn as nn
from torch.nn.utils.rnn import pack_padded_sequence as pack
from torch.nn.utils.rnn import pad_packed_sequence as unpack
```

<br/>

## Encoder
![seq2seq(9)](/assets/images/seq2seq복습/seq2seq(9).jpg)  

```{.python}
class Encoder(nn.Module):
  
  def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_p=2):
    '''
    word_vec_size : 단어의 임베딩 크기
    hidden_size: LSTM의 출력 사이즈
    n_layers: LSTM 층의 개수
    dropout_p: drop out 비율
    '''
    super(Encoder, self).__init__()
    
    # bi-directional LSTM을 사용하여 양 방향의 결과를 이어 붙일 것이므로, LSTM의 hidden size를 hidden_size/2로 설정
    # batch_first=True로 설정하여 텐서의 첫번째 차원이 배치 크기가 되도록 함
    self.rnn = nn.LSTM(
      word_vec_size,
      int(hidden_size / 2),
      num_layers = n_layers,
      bidirectional=True,
      batch_first=True,
      )
     
      def forward(self, emb):
        # |emb| = (batch_size, length:문장의 길이, word_vec_size)
        
        # 미니 배치 별로 문장 길이에 따라 문장을 재정렬함으로써 <PAD> 토큰 계산 안하고 빠른 병렬 연산 수행
        # 참고(Pytorch 의 PackedSequence object 알아보기): https://simonjisu.github.io/nlp/2018/07/05/packedsequence.html
        if isinstance(emb, tuple):
          # 미니 배치 별로 각 문장과 길이를 받아서 pack 한다
          x, length = emb
          x = pack(x, lengths.tolist(), batch_first=True)
        else:
          x = emb
        
        # If (h_0, c_0) is not provided, both h_0 and c_0 default to zero.
        y, h = self.rnn(x)
        # |y| = (batch_size, length, hidden_size)
        # -> 미니 배치별로 각 문장의 단어(time step) 별 (마지막 레이어를 거친 최종) 히든 스테이트를를 담고 있음
        # |h[0]| = (num_layers*2, batch_size, hidden_size/2)
        # -> h=(hidden, cell)이므로 h[0]은 미니 배치별, 각 레이어 별 모든 히든 스테이트를 담고 있음
        
        # 인풋이 packed된 형태라면 아웃풋도 packed된 형태이므로 풀어줌
        if isinstance(emb, tuple):
          y, _ = unpack(y, batch_first=True)
          
         return y, h
```

<br/>

## Attention
![seq2seq(8)](/assets/images/seq2seq복습/seq2seq(8).jpg)

```{.python}
class Attention(nn.Module):

  def __init__(self, hidden_size):
  '''
  hidden_size: LSTM의 출력 사이즈
  '''
  super(Attention, self).__init__()
  
  # self.linear를 통해 적절한 쿼리를 던질 수 있는 선형변환을 학습시킴
  self.linear = nn.Linear(hidden_size, hidden_size, bias=False)
  self.softmax = nn.Softmax(dim=-1)
  
  def forward(self, h_src, h_t_tgt, mask=None):
    # |h_src| = (batch_size, length, hidden_size)
    # |h_t_tgt| = (batch_size, 1, hidden_size)
    # |mask| = (batch_size, length)
    
    # 디코더의 각 단어를 넣어서 인코더에 보낼 쿼리 생성
    query = self.linear(h_t_tgt)
    # |query| = (batch_size, 1, hidden_size)
    
    # 인코더 각 단어에 곱할 attention weight
    weight = torch.bmm(query, h_src.transpose(1, 2))
    # |weight| = (batch_size, 1, length)
    
    # <PAD> 토큰에 해당하는 부분을 -inf로 바꾸어서 소프트맥스 출력값이 0이 되게 함
    # 패딩 부분에 대한 weight을 0으로 만들기
    if mask in not None:
      weight.masked_fill_(mask.unsqueeze(1), -float('inf'))
    
    weight = self.softmax(weight)
    
    # 인코더 각 단어에 attention weight을 곱해서 디코더 각 단어에 대한 context_vector 만들기
    # 인코더 단어 중 디코더 각 단어와 관련이 높은 단어는 많이, 낮은 단어는 적게 반영 
    context_vector = torch.bmm(weight, h_src)
    
    return context_vector
    
```

<br/>

## Decoder
![seq2seq(10)](/assets/images/seq2seq복습/seq2seq(10).jpg)

```{.python}
class Decoder(nn.Module):
  
  def __init__(self, word_vec_size, hidden_size, n_layers=4, dropout_o=.2):
    
    super(Decoder, self).__init__()
    
    # input 사이즈는 디코더 단어의 임베딩 차원 + context_vector의 크기(hidden_size)
    self.rnn = nn.LSTM(
      word_vec_size + hidden_size,
      hidden_size,
      num_layers = n_layers,
      dropout = dropout_p,
      bidirectional=False,
      batch_first=True,
      )
      
    def forward(self, emb_t, h_t_1_tilde, h_t_1):
      # 인코더에서는 단어 별로 별도의 처리 과정이 없기에 문장의 단어 전체를 한 번에 모델에 넣어주지만, 
      # 디코더에서는 이전 hidden state를 concat하는 과정이 필요하므로 단어 한 개씩 처리
      # |emb_t| = (batch_size, 1, word_vec_size)
      # |h_t_1_tilde| = (batch_size, 1, hidden_size)
      # |h_t_1[0]| = (n_layers, batch_size, hidden_size)
      
      batch_size = emb_t.size(0)
      hidden_size = h_t_1[0].size(-1)
      
      # 인코더가 끝나고 디코더가 시작되는 부분
      # emb_t와 같은 타입, 같은 디바이스에 있는 텐서로 만들기
      if h_t_1_tilde i None:
        h_t_1_tilde = emb_t.new(batch_size, 1, hidden_size).zero()
        
      # teacher forcing: 인풋으로 실제 정답을 넣음
      # 이전 스텝의 추론 결과를 반영하기 위해 이전 스텝의 hidden state(ht_t_1_tilde)를 넣음
      x = torch.cat([emb_t, ht_t_1_tilde], dim=-1)
      
      y, h = self.rnn(x, h_t_1)
      
      return y, h
```

<br/>

## Generator
![seq2seq(11)](/assets/images/seq2seq복습/seq2seq(11).jpg)


```{.python}
class Generator(nn.Module):
    
    def __init__(self, hidden_size, output_size):
        
        super(Generator, self).__init__()
        
        self.output = nn.Linear(hidden_size, output_size)
        # 왜 Softmax 대신에 LogSoftmax를 쓰는지 궁금해서 찾아봄
        # https://datascience.stackexchange.com/questions/40714/what-is-the-advantage-of-using-log-softmax-instead-of-softmax
        self.softmax = nn.LogSoftmax(dim=-1)
        # 대표적인 이유로, 1. 경사 계산 할 때 더 안정적이고 2. 예측 잘못 되었을 때 더 큰 패널티 부여 가능
        
     # inference와 달리 train 단계의 generator는 문장의 모든 단어 전체를 넣고 한 번에 진행
     # generation을 위해 h_t_1_tilde가 필요한거지, 최종 예측값이 필요한 것은 아니기 때문에 한 문장 내에서 각 단어의 hidden state 한 번에 넣음
     def forward(self, x):
     # |x| = (batch_size, length, hidden_size)
     
     y = self.softmax(self.output(x))
     # |y| = (batch_size, length, output_size)
     
     return y
```


<br/>

## Seq2Seq(최종)

```{.python}
class Seq2Seq(nn.Module):

    def __init__(
        self,
        input_size,
        word_vec_size,
        hidden_size,
        n_layers=4,
        drop_out_p=.2
    ):
        self.input_size = input_size
        self.word_vec_size = word_vec_size
        self.hidden_size = hidden_size
        self.output_size = output_size
        self.n_layers = n_layers
        self.dropout_p = dropout_p
        
        super(Seq2Seq, self).__init__()
        
        self.emb_src = nn.Embedding(input_size, word_vec_size)
        self.emb_dec = nn.Embedding(output_size, word_vec_size)
        
        self.encoder = Enconder(
            word_vec_size, hidden_size,
            n_layers=n_layers, dropout_p=dropout_p,
            )
        self.decoder = Decoder(
            word_vec_size, hidden_size,
            n_layers=n_layers, dropout_p=dropout_p,
            )
        self.attn = Attention(hidden_size)
        
        # context vector와 현재 hidden_vector 합쳐서 h_t_tilde 만들고 Generator에 넣음
        self.concat = nn.Linear(hidden_size*2, hidden_size)
        self.tanh = nn.Tanh()
        self.generator = Generator(hidden_size, ouput_size)
        
   
    # <PAD> 토큰을 True, 나머지를 False로 마스킹
    def generate_mask(self, x, length):
        # |mask| = (batch_size, length)
        # |x| = (batch_size, length)
        # length = (batch_size, )
        mask = []
        
        max_length = max(length)
        for l in length:
            # 최대 문장 길이 보다 짧은 문장에 대해 masking 진행
            if max_length - l > 0:
                mask += [torch.cat([x.new_ones(1, l).zero_(),
                                    x.new_onews(1, max_lenght -l))
                                    ], dim=-1)]
            else:
                mask += [x.new_onew(1, l).zero_()]
        
        mask = torch.cat(mask, dim=0).bool()
        
        return mask
        
    
    # 실제로는 아래의 fast_merge_encoder_hiddens 사용
    # 'merge_encoder_hiddens' method works with non-parallel way.
    def merge_encoder_hiddens(self, encoder_hiddens):
        new_hiddens = []
        new_cells =[]
        
        # |hiddens| = (#layers x 2, batch_size, hidden_size / 2)
        hiddens, celss = encoder_hiddens
        
        # bi-directional 이어서 i번째와 (i+1)번째 레이어는 같은 타임스텝의 방향만 다른 레이어
        for i in range(0, hiddens.size(0), 2): 
            new_hiddens += [torch.cat([hiddens[i], hiddens[i+1]], dim=-1)]
            new_cells += [torch.cat([celss[i], cells[i+1]], dim=-1)]
            
        # |new_hiddens| = (#layers, batch_size, hidden_size)
        
        return (new_hiddens, new_cells)
        
    
    # for문 쓰지 않고 한꺼번에 모양 변경
    def fast_merge_encoder_hiddens(self, encdoer_hiddens):
        # |h_0_tgt| = (#layers x 2, batch_size, hidden_size / 2)
        h_0_tgt, c_0_tgt = encoder_hiddens
        batch_size = h_0_tgt.size(1)
        
        #  transpose(0, 1) -> (batch_size, #layers x 2, hidden_size / 2)
        # view -> (batch_size, #layers, hidden_size)
        h_0_tgt = h_0_tgt.transpose(0, 1).contiguous().view(batch_size,
                                                            -1,
                                                            self.hidden_size
                                                            ).transpose(0, 1).contiguous()
        c_0_tgt = c_0_tgt.transpose(0, 1).contiguous().view(batch_size,
                                                            -1,
                                                            self.hidden_size
                                                            ).transpose(0, 1).contiguous()
        return h_0_tgt, c_0_tgt
        
    
    # 학습할 때는 타켓 문장도 입력(teacher forcing)
    def forward(self, src, tgt):
        mask = None
        x_length = None
        
        # step1. 마스킹 
        if isinstance(src, tuple):
            x, x_length = src
            mask = self.generate_mask(x, x_length)
            # |mask| = (batch_size, length)
        else:
            x = src
        
        if isinstance(tgt, tuple):
            tgt = tgt[0]
            
        # step2. 인코딩 인풋 임베딩
        emb_src = self.emb_src(x)
        # |emb_src| = (batch_size, length, word_vec_size)
        
        # step3. 인코더
        # 문장과 문장의 길이를 함께 넣어서 packing 가능하게 함
        h_src, h_0_tgt = self.encoder((emb_src, x_length))
        # |h_src| = (batch_size, length, hidden_size)
        # |h_0_tgt| = (n_layers * 2, batch_size, hidden_size/2)
        
        # step4. 인코더의 마지막 hidden_state를 디코더의 첫번째 hidden_state로 넘기기
        h_0_tgt = self.fast_merge_encoder_hiddens(h_0_tgt)
        decoder_hidden = h_0_tgt
        
        # step5. 디코더 인풋 임베딩
        emb_tgt = self.emb_dec(tgt)
        # |emb_tgt| = (batch_size, length, word_vec_size)
        
        # step6. 디코더(디코더는 단어 단위로 진행 / teacher forcing 사용)
        h_tilde = []
        h_t_tilde = None
        
        for t in range(tgt.size(1)):
            # 1차원으로 인덱싱하면 (batch_size, word_vec_size)로 모양이 바뀌는데, 헷갈리니까 1차원 다시 추가해준다.
            emb_t = emb_tgt[:, t, :].unsqueeze(1)
            # |emb_t| = (batch_size, 1, word_vec_size)
            # |h_t_tilde| = (batch_size, 1, hidden_size)
            
            # step6-1. 디코더 단어 별 LSTM output 구하기
            decoder_output, decoder_hidden = self.decoder(emb_t,
                                                          h_t_tilde,
                                                          decoder_hidden
                                                          )
            # |decoder_output| = (batch_size, 1, hidden_size)
            # |decoder_hidden| = (n_layers, batch_size, hidden_size)
            
            # step6-2. 디코더 단어 별 LSTM output을 쿼리로 보내서 context_vector 구하기 
            context_vector = self.attn(h_src, decoder_output, mask)
            # |context_vector| = (batch_size, 1, hidden_size)
            
            # step6-3. 디코더 단어 별 LSTM ouput과 context vector를 이어 붙여서 h_tilde 구하기
            h_t_tilde = self.tanh(self.concat(torch.cat([decoder_output,
                                                         context_vector
                                                         ], dim=-1)))
            # |h_t_tilde| = (batch_size, 1, hidden_size)
            
            # 한번에 Generator에 보내기 위해 h_tilde 모아두기
            h_tilde += [h_t_tilde]
        
        h_tilde = torch.cat(h_tilde, dim=1)
        # |h_tilde| = (batch_size, length, output_size)
        
        return y_hat
                                                          
      
```

